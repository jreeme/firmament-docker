version: '3.3'
networks:
  net:
services:
  #HDFS and Spark Name Node Service
  namenode:
    image: 52.0.211.45:5000/hadoop-namenode
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == manager
    networks:
      - net
  #  volumes:
  #    - $PWD/../share:/share
    ports:
      - "8088:8088"
      - "50070:50070"
      - "9000:9000"
      - "8042:8042"
      - "8020:8020"

  #HDFS Data Nodes
  datanode:
    image: 52.0.211.45:5000/hadoop-base
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
    networks:
      - net
  #Hadoop/spark workers on Yarn
  sparkworker:
    image: 52.0.211.45:5000/hadoop-base
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
    command: "/root/hadoop_entrypoint.sh -spark_worker"
    networks:
      - net
  portainer:
    image: 52.0.211.45:5000/portainer:1.16.2
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == manager
    ports:
      - '9010:9000'
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - net
  postgres:
    image: 52.0.211.45:5000/postgres:9.5.10
    environment:
      - POSTGRES_PASSWORD=password
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == manager
    ports:
      - '5432:5432'
    networks:
      - net
  amino3:
    image: 52.0.211.45:5000/amino3:1.1
    environment:
      - NODE_ENV=production
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == manager
    ports:
      - '3000:3000'
    networks:
      - net
  nginx:
    image: 52.0.211.45:5000/nginx:1.13.8
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == manager
    ports:
        - '9200:9200'
    networks:
      - net
  #HDFS and Spark Name Node Service
  elasticsearch:
    image: 52.0.211.45:5000/elasticsearch:5.6.6
    deploy:
      replicas: 3
      endpoint_mode: dnsrr
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == manager
#    ports:
#      - "9200:9200"
#      - "9300:9300"
    #command: "elasticsearch -Enetwork.host=0.0.0.0 -Ediscovery.zen.ping.unicast.hosts=elasticsearch -Ediscovery.zen.minimum_master_nodes=1"
    networks:
      - net
  elasticsearch-head:
    image: 52.0.211.45:5000/elasticsearch-head:5.x
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == manager
    ports:
      - "9100:9100"
    networks:
      - net
